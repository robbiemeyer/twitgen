{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJfLuIaWdHm_"
   },
   "source": [
    "# Twitgen\n",
    "\n",
    "A basic text generator designed to work with tweets. Bases off of: https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "This is a work in progress. This notebook contains inital efforts to generate tweets from four different users.\n",
    "\n",
    "https://github.com/taspinar/twitterscraper was used to generate tweet json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6jbBB6xcHHD"
   },
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojk_HWct8UAX"
   },
   "source": [
    "## Preparing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l11Nwa1J4Czi"
   },
   "outputs": [],
   "source": [
    "with open('trump_tweets_post2017.json') as data:\n",
    "    raw_tweets = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkA4Lte7ONHU"
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r\"(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)|(pic\\.twitter\\.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)\")\n",
    "extract_tweet = lambda tweet: regex.sub('', tweet['text']).strip()\n",
    "is_not_empty = lambda s: len(s) > 0\n",
    "tweet_text = list(filter(is_not_empty, map(extract_tweet, raw_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFwxisqJOQRD"
   },
   "outputs": [],
   "source": [
    "vocab = ['END_TWEET'] + sorted(set(''.join(tweet_text)))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UE6qir_tOTe_"
   },
   "outputs": [],
   "source": [
    "text_as_int = []\n",
    "for tweet in tweet_text:\n",
    "    text_as_int.extend([char2idx[c] for c in tweet])\n",
    "    text_as_int.append(0)\n",
    "text_as_int = np.array(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_TBMV71duXC"
   },
   "outputs": [],
   "source": [
    "# Create training examples / targets\n",
    "seq_length = 32\n",
    "examples_per_epoch = len(text_as_int)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ScALO8ald7C8"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "# Batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqSTepXJfLI0"
   },
   "source": [
    "## Building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rk5YaykEeHC3"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDGAuSwifRx_"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,\n",
    "                              embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform',\n",
    "                        dropout=0.2),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "training_model = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkVaRuNAgoR_"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "training_model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "US6aTydyhBgN"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SX2fGVpNgObr",
    "outputId": "31c8cfa8-005e-4563-9491-d0128c2ef854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "172/172 [==============================] - 9s 52ms/step - loss: 2.9199\n",
      "Epoch 2/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 2.3541\n",
      "Epoch 3/40\n",
      "172/172 [==============================] - 6s 36ms/step - loss: 2.1171\n",
      "Epoch 4/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 1.8947\n",
      "Epoch 5/40\n",
      "172/172 [==============================] - 6s 36ms/step - loss: 1.7011\n",
      "Epoch 6/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 1.5240\n",
      "Epoch 7/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 1.3668\n",
      "Epoch 8/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 1.2213\n",
      "Epoch 9/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 1.0856\n",
      "Epoch 10/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.9800\n",
      "Epoch 11/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.8889\n",
      "Epoch 12/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.8142\n",
      "Epoch 13/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.7566\n",
      "Epoch 14/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.7101\n",
      "Epoch 15/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.6730\n",
      "Epoch 16/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.6396\n",
      "Epoch 17/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.6233\n",
      "Epoch 18/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.6051\n",
      "Epoch 19/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5879\n",
      "Epoch 20/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5744\n",
      "Epoch 21/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5587\n",
      "Epoch 22/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5463\n",
      "Epoch 23/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5439\n",
      "Epoch 24/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5337\n",
      "Epoch 25/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5295\n",
      "Epoch 26/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5271\n",
      "Epoch 27/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5167\n",
      "Epoch 28/40\n",
      "172/172 [==============================] - 6s 36ms/step - loss: 0.5184\n",
      "Epoch 29/40\n",
      "172/172 [==============================] - 6s 36ms/step - loss: 0.5079\n",
      "Epoch 30/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5013\n",
      "Epoch 31/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5076\n",
      "Epoch 32/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.5005\n",
      "Epoch 33/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.4989\n",
      "Epoch 34/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.4985\n",
      "Epoch 35/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.4980\n",
      "Epoch 36/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.4935\n",
      "Epoch 37/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.4943\n",
      "Epoch 38/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.4931\n",
      "Epoch 39/40\n",
      "172/172 [==============================] - 6s 36ms/step - loss: 0.4908\n",
      "Epoch 40/40\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 0.4891\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=40\n",
    "history = training_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LuI_5k9N4NoJ"
   },
   "outputs": [],
   "source": [
    "weights = training_model.get_weights()\n",
    "predict_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "predict_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZ7NYFiOiysO"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, idx2char, temperature=1):\n",
    "  # Number of characters to generate\n",
    "  num_generate = 280\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = tf.expand_dims([0], 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  model.reset_states()\n",
    "  n = 0\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / (0.5**n * temperature)\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[0,0].numpy()\n",
    "\n",
    "      if predicted_id == 0:\n",
    "        return ''.join(text_generated)\n",
    "\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "      n = 0 if idx2char[predicted_id] == ' ' else n + 1\n",
    "\n",
    "  return ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### @realdonaldtrump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "IczlQx7knyJ9",
    "outputId": "596aa3a5-0b6e-4466-8793-95ac477d9d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White House, with the United States will not allow other won the amazing In many ways this is the successful model we will make a BIG difference! \n",
      "\n",
      "Thank you to everyone at @reat to protect y from Tuesday. A false narrative that she was great day in Puerto Rico failed. I won't fail. \n",
      "\n",
      "White House, Virginia against the Impeachment Proceeding, who should never ending Witch Hunt, look locked down, no ran RobiDed What’s going on? \n",
      "\n",
      "Will be heading he The lleting so many years and all sorts to mention that the candidate Impeachment Proceeding, and have your back. We will ALWAYS be wasted! \n",
      "\n",
      "Italy, @GiuseppeConteIT, a really great guy who can fight for Healthcare but not for Friday! \n",
      "\n",
      "Thank you to PERDON mys for the obviously needed Wall (they overrode recommendations of Border Patrol experts), but they don’t even want to take muderers into custody! What’s going on? \n",
      "\n",
      "My Administration is nowhich abuts and is part of the United States Coasting, when it recommendations of law. Great reviews from 4 years ago! \n",
      "\n",
      "I inherited)! \n",
      "\n",
      "Thank you to General Motors should focus their energies on ISIS, illegal immigration representatives and I said Green Party candidate, Jill Stein, or sted @NBCNews went out to be extremely passive and nations. If we charge us up and have some fun. The Country dead (and many for a \n",
      "\n",
      "I think it right for presidential candidates John Crooked Hillary Clinton just called the committee has direct access to Intelligence community. Have done Border Patrol experts), when people of Italy got you to Get Out and Vote for Tate Reeves President, why poor the people of Ke \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(generate_text(predict_model, idx2char, temperature=1.2), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOIb8fxEiEju"
   },
   "outputs": [],
   "source": [
    "predict_model.save('trump.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vR6cQi9yNtEL"
   },
   "outputs": [],
   "source": [
    "np.save('trump_idx2char.npy', idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21xjBYDaULuT"
   },
   "source": [
    "## Loading Models based on other users\n",
    "\n",
    "#### @arianagrande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugEB_DsdUDGi"
   },
   "outputs": [],
   "source": [
    "ariana_model = tf.keras.models.load_model('arianagrande.h5', compile=False)\n",
    "idx2char = np.load('arianagrande_idx2char.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "QTF-pi_QQawH",
    "outputId": "6e7a5ef4-5111-40d5-997d-8680d643490b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't wait you’ll ever hear a live album one  for everything. \n",
      "\n",
      "five doys til dwt? \n",
      "\n",
      "ᶦman Tour \n",
      "\n",
      "♡ I LOVE U SO MUCH … \n",
      "\n",
      "five days til sweetener preorder and tlic ♡ \n",
      "\n",
      "yooooo \n",
      "\n",
      "she LOVES it. … \n",
      "\n",
      "#mandest, funniest, brightest light. i can’t wait to spend more time togetha. … \n",
      "\n",
      "25%  til dwt? more. i love you. \n",
      "\n",
      ") ♡  … \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(generate_text(ariana_model, idx2char, temperature=1.4), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### @BBCWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xq4kRAhuVLH-"
   },
   "outputs": [],
   "source": [
    "bbc_model = tf.keras.models.load_model('bbc.h5', compile=False)\n",
    "idx2char = np.load('bbc_idx2char.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "colab_type": "code",
    "id": "m2JtKXdPVRPu",
    "outputId": "d9030f35-d060-4869-cce4-472b5ac8ee71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least 11 killed as fast-spreames Puiwdemont   #10Oct \n",
      "\n",
      "Trump aide Kellyanne Conway say \"I where people attempted a cuments from buses\n",
      "\n",
      "We are so grateful country!\"\n",
      "\n",
      "Thousands of families, tourists and merchants are queuing every day to return for Robbie Butina deported from Abries $2018: Is to 'The day I was diagnosed was the sea — w \n",
      "\n",
      "Steenhuisen to head South Africa's cash-in-trafficking trial starts in French kitchen deal diegnts attempt an ,uetallegat rules to 'restore tranquillity' \n",
      "\n",
      "Trump protests: LGBTQ rally in New York \n",
      "\n",
      "Catalonia: Spain head hours @\n",
      "\n",
      "[tap to expand] \n",
      "\n",
      "Katie Hill: House ban not backing down after decree repeal \n",
      "\n",
      "Zara advert gets China asking: Are freckles beautiful? \n",
      "\n",
      "Zara out a living in Haiti's what our son Noah chat park day to return to Syria\n",
      "\n",
      "[tap to most prolific figures, Karina election: Polls open as voters choose between Cuntay marked the 11th day of protests in the country, where people attempted a human camera \n",
      "\n",
      "Trump protests: LGBTQ rally in New York \n",
      "\n",
      "\"He keeps screaming, 'drain the plas on awainst IS 'cheating husbands' are linked the Bosnian refugee who flees scene conflict: Rebel commander killer' dies aged 74mped hingya abuse in Myanmar \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(generate_text(bbc_model, idx2char, temperature=1.4), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### @Wendys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgYqaO0VeBx"
   },
   "outputs": [],
   "source": [
    "wendys_model = tf.keras.models.load_model('wendys.h5', compile=False)\n",
    "idx2char = np.load('wendys_idx2char.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "yxqAwyCbViIX",
    "outputId": "5a8c2e77-9622-4b8c-f370-4c6884ad4cf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's not okay. Thanks! \n",
      "\n",
      "Qunchise ry your number, and we'll make it up to you. \n",
      "\n",
      "Jason is the best! M us with info on this location, along with your number, and we'll make it up to you. \n",
      "\n",
      "Just as up to you. \n",
      "\n",
      "IThat's not okay! Please DM us the restaurant location and your phone # so we can make this right. \n",
      "\n",
      "That's not okay. Please DM us with info on this location, along with your phone number, and we'll make it up to you. \n",
      "\n",
      "We're disappointed your phone # so we can make this up to you. \n",
      "\n",
      "Our Breakfast is delicious and names it after er. \n",
      "\n",
      "Wwwww like to hear that! Please DM us your email address so we for maing address so we can improve. Thank you! \n",
      "\n",
      "It maybe not eating. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(generate_text(wendys_model, idx2char, temperature=1.4), '\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tf_text_generation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
